{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.5.1-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.23.5-cp310-cp310-macosx_11_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m290.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Using cached pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.23.5 pandas-1.5.1 pytz-2022.6\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp310-cp310-macosx_12_0_arm64.whl (3.7 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m356.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp310-cp310-macosx_11_0_arm64.whl (287 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: tokenizers, urllib3, typing-extensions, regex, pyyaml, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
      "Successfully installed certifi-2022.9.24 charset-normalizer-2.1.1 filelock-3.8.0 huggingface-hub-0.11.0 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 tokenizers-0.13.2 transformers-4.24.0 typing-extensions-4.4.0 urllib3-1.26.12\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post1-py3-none-any.whl\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post1\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.0-cp310-none-macosx_11_0_arm64.whl (55.7 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.0\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.3-cp310-cp310-macosx_12_0_arm64.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m60.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0mm\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.3-cp310-cp310-macosx_12_0_arm64.whl (28.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting joblib>=1.0.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.1.3 scipy-1.9.3 threadpoolctl-3.1.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.7.0-py3-none-any.whl (451 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m451.6/451.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Using cached xxhash-3.1.0-cp310-cp310-macosx_11_0_arm64.whl (31 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (0.11.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: packaging in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.0-cp310-cp310-macosx_11_0_arm64.whl (22.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.6/22.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from datasets) (1.5.1)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp310-cp310-macosx_11_0_arm64.whl (336 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.1-cp310-cp310-macosx_11_0_arm64.whl (57 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-macosx_11_0_arm64.whl (34 kB)\n",
      "Requirement already satisfied: filelock in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/huangshenhua/Documents/lab/DMcourse/lab2/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.7.0 dill-0.3.6 frozenlist-1.3.3 fsspec-2022.11.0 multidict-6.0.2 multiprocess-0.70.14 pyarrow-10.0.0 responses-0.18.0 xxhash-3.1.0 yarl-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install pandas\n",
    "# ! pip3 install transformers \n",
    "# ! pip3 install sklearn\n",
    "# ! pip3 install torch\n",
    "# ! pip3 install scikit-learn\n",
    "# ! pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-21T04:07:44.958360Z",
     "iopub.status.busy": "2022-11-21T04:07:44.957715Z",
     "iopub.status.idle": "2022-11-21T04:07:44.996602Z",
     "shell.execute_reply": "2022-11-21T04:07:44.995463Z",
     "shell.execute_reply.started": "2022-11-21T04:07:44.958248Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:07:44.999294Z",
     "iopub.status.busy": "2022-11-21T04:07:44.998951Z",
     "iopub.status.idle": "2022-11-21T04:08:03.160278Z",
     "shell.execute_reply": "2022-11-21T04:08:03.159047Z",
     "shell.execute_reply.started": "2022-11-21T04:07:44.999266Z"
    }
   },
   "outputs": [],
   "source": [
    "r=[]\n",
    "d={}\n",
    "with open('./data/tweets_DM.json',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        x = json.loads(line)\n",
    "#         assert x['_source']['tweet']['tweet_id'] not in d , 'oh shit'\n",
    "#         d[x['_source']['tweet']['tweet_id']] = {\n",
    "#             \"_score\": x['_score'] , \n",
    "#             \"_index\": x['_index'],\n",
    "#             \"hashtags\": x['_source']['tweet'][\"hashtags\"],\n",
    "#             \"text\": x['_source'][\"tweet\"]['text'],\n",
    "#             \"_crawldate\":x['_crawldate'],\n",
    "#             \"_type\":x[\"_type\"]\n",
    "#         }\n",
    "        r.append ([ x['_source']['tweet']['tweet_id'],\n",
    "                 x['_score'] , \n",
    "                 x['_index'],\n",
    "                 x['_source']['tweet'][\"hashtags\"],\n",
    "                 x['_source'][\"tweet\"]['text'],\n",
    "                 x['_crawldate'],\n",
    "                 x[\"_type\"]\n",
    "            ])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:08:03.161813Z",
     "iopub.status.busy": "2022-11-21T04:08:03.161458Z",
     "iopub.status.idle": "2022-11-21T04:08:04.441440Z",
     "shell.execute_reply": "2022-11-21T04:08:04.440231Z",
     "shell.execute_reply.started": "2022-11-21T04:08:03.161780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>827</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>2015-05-12 12:51:52</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>368</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>2017-10-02 17:54:04</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>498</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>2016-10-10 11:04:32</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>840</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>2016-09-02 14:25:06</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>360</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>2016-11-16 01:40:07</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  _score          _index                         hashtags  \\\n",
       "0        0x376b20     391  hashtag_tweets                       [Snapchat]   \n",
       "1        0x2d5350     433  hashtag_tweets    [freepress, TrumpLegacy, CNN]   \n",
       "2        0x28b412     232  hashtag_tweets                     [bibleverse]   \n",
       "3        0x1cd5b0     376  hashtag_tweets                               []   \n",
       "4        0x2de201     989  hashtag_tweets                               []   \n",
       "...           ...     ...             ...                              ...   \n",
       "1867530  0x316b80     827  hashtag_tweets  [mixedfeeling, butimTHATperson]   \n",
       "1867531  0x29d0cb     368  hashtag_tweets                               []   \n",
       "1867532  0x2a6a4f     498  hashtag_tweets                               []   \n",
       "1867533  0x24faed     840  hashtag_tweets                               []   \n",
       "1867534  0x34be8c     360  hashtag_tweets                    [Sundayvibes]   \n",
       "\n",
       "                                                      text  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        Confident of your obedience, I write to you, k...   \n",
       "3                      Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4        \"Trust is not the same as faith. A friend is s...   \n",
       "...                                                    ...   \n",
       "1867530  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  I swear all this hard work gone pay off one da...   \n",
       "1867532  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                  _crawldate   _type  \n",
       "0        2015-05-23 11:42:47  tweets  \n",
       "1        2016-01-28 04:52:09  tweets  \n",
       "2        2017-12-25 04:39:20  tweets  \n",
       "3        2016-01-24 23:53:05  tweets  \n",
       "4        2016-01-08 17:18:59  tweets  \n",
       "...                      ...     ...  \n",
       "1867530  2015-05-12 12:51:52  tweets  \n",
       "1867531  2017-10-02 17:54:04  tweets  \n",
       "1867532  2016-10-10 11:04:32  tweets  \n",
       "1867533  2016-09-02 14:25:06  tweets  \n",
       "1867534  2016-11-16 01:40:07  tweets  \n",
       "\n",
       "[1867535 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(r , columns=['tweet_id', '_score' ,'_index' ,'hashtags' , 'text' ,'_crawldate', \"_type\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:08:04.445127Z",
     "iopub.status.busy": "2022-11-21T04:08:04.444154Z",
     "iopub.status.idle": "2022-11-21T04:08:16.476714Z",
     "shell.execute_reply": "2022-11-21T04:08:16.475570Z",
     "shell.execute_reply.started": "2022-11-21T04:08:04.445089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>_score</th>\n",
       "      <th>text</th>\n",
       "      <th>_crawldate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>joy</td>\n",
       "      <td>809</td>\n",
       "      <td>Huge RespectðŸ–’ @JohnnyVegasReal talking about l...</td>\n",
       "      <td>2015-01-17 03:07:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>joy</td>\n",
       "      <td>808</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "      <td>2016-07-02 09:34:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>trust</td>\n",
       "      <td>16</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "      <td>2016-08-15 18:18:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>joy</td>\n",
       "      <td>768</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "      <td>2017-02-11 08:49:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>70</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "      <td>2016-11-23 05:37:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>disgust</td>\n",
       "      <td>361</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "      <td>2016-09-09 14:28:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>sadness</td>\n",
       "      <td>15</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "      <td>2017-02-04 14:15:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>joy</td>\n",
       "      <td>174</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "      <td>2015-12-03 16:53:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>trust</td>\n",
       "      <td>515</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "      <td>2016-10-27 03:23:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>trust</td>\n",
       "      <td>850</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "      <td>2016-08-26 08:41:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       emotion  _score  \\\n",
       "0        0x29e452           joy     809   \n",
       "1        0x2b3819           joy     808   \n",
       "2        0x2a2acc         trust      16   \n",
       "3        0x2a8830           joy     768   \n",
       "4        0x20b21d  anticipation      70   \n",
       "...           ...           ...     ...   \n",
       "1455558  0x227e25       disgust     361   \n",
       "1455559  0x293813       sadness      15   \n",
       "1455560  0x1e1a7e           joy     174   \n",
       "1455561  0x2156a5         trust     515   \n",
       "1455562  0x2bb9d2         trust     850   \n",
       "\n",
       "                                                      text  \\\n",
       "0        Huge RespectðŸ–’ @JohnnyVegasReal talking about l...   \n",
       "1        Yoooo we hit all our monthly goals with the ne...   \n",
       "2        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n",
       "3        Come join @ambushman27 on #PUBG while he striv...   \n",
       "4        @fanshixieen2014 Blessings!My #strength little...   \n",
       "...                                                    ...   \n",
       "1455558  @BBCBreaking Such an inspirational talented pe...   \n",
       "1455559  And still #libtards won't get off the guy's ba...   \n",
       "1455560  When you sow #seeds of service or hospitality ...   \n",
       "1455561  @lorettalrose Will you be displaying some <LH>...   \n",
       "1455562                               Lord, I <LH> in you.   \n",
       "\n",
       "                  _crawldate  \n",
       "0        2015-01-17 03:07:03  \n",
       "1        2016-07-02 09:34:06  \n",
       "2        2016-08-15 18:18:39  \n",
       "3        2017-02-11 08:49:46  \n",
       "4        2016-11-23 05:37:10  \n",
       "...                      ...  \n",
       "1455558  2016-09-09 14:28:19  \n",
       "1455559  2017-02-04 14:15:32  \n",
       "1455560  2015-12-03 16:53:39  \n",
       "1455561  2016-10-27 03:23:51  \n",
       "1455562  2016-08-26 08:41:46  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name= pd.read_csv(\"./data/data_identification.csv\")\n",
    "class_name\n",
    "train = class_name.query(\"identification == 'train'\")\n",
    "test = class_name.query(\"identification == 'test'\")\n",
    "\n",
    "class_emotion= pd.read_csv(\"./data/emotion.csv\")\n",
    "\n",
    "\n",
    "x = pd.merge(train , class_emotion)\n",
    "train_data = pd.merge(x,y)\n",
    "train_data = train_data.drop(columns=['identification' , 'hashtags' ,'_type' , '_index' ])\n",
    "train_data.to_pickle('td.pkl')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:08:16.478420Z",
     "iopub.status.busy": "2022-11-21T04:08:16.478117Z",
     "iopub.status.idle": "2022-11-21T04:08:24.243430Z",
     "shell.execute_reply": "2022-11-21T04:08:24.242065Z",
     "shell.execute_reply.started": "2022-11-21T04:08:16.478396Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(dataslice):\n",
    "\n",
    "    \n",
    "    # [ TODO ] use your tokenizor and encoder to get sentence embeddings and encoded labels\n",
    "    tok = tokenizer(dataslice[\"text\"])\n",
    "    tmp = labelencoder.fit_transform(dataslice['emotion'])\n",
    "    label = encoder.fit_transform(tmp.reshape(-1,1)).toarray()\n",
    "    tok['label'] = label\n",
    "    return tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:08:24.245423Z",
     "iopub.status.busy": "2022-11-21T04:08:24.244981Z",
     "iopub.status.idle": "2022-11-21T04:08:25.605656Z",
     "shell.execute_reply": "2022-11-21T04:08:25.604777Z",
     "shell.execute_reply.started": "2022-11-21T04:08:24.245379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "tt = labelencoder.fit_transform(train_data[\"emotion\"])\n",
    "encoder.fit_transform(tt.reshape(-1,1))\n",
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:08:25.608024Z",
     "iopub.status.busy": "2022-11-21T04:08:25.606864Z",
     "iopub.status.idle": "2022-11-21T04:09:47.412984Z",
     "shell.execute_reply": "2022-11-21T04:09:47.411802Z",
     "shell.execute_reply.started": "2022-11-21T04:08:25.607988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6ec197372948a1d5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset pandas/default to /Users/huangshenhua/.cache/huggingface/datasets/pandas/default-6ec197372948a1d5/0.0.0/3ac4ffc4563c796122ef66899b9485a3f1a977553e2d2a8a318c72b8cc6f2202...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a069f818774159b8bcdd733128e13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc0c7b2bd02461da81477d925d66a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pandas downloaded and prepared to /Users/huangshenhua/.cache/huggingface/datasets/pandas/default-6ec197372948a1d5/0.0.0/3ac4ffc4563c796122ef66899b9485a3f1a977553e2d2a8a318c72b8cc6f2202. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf472c8523f433a80d3a295d90131e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ce39cfefe44585a1ee1f964e43bb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1456 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('pandas' , data_files='td.pkl')\n",
    "\n",
    "processed_data = dataset.map(preprocess,    # your processing function\n",
    "                             batched = True # Process in batches so it can be faster\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:09:47.416605Z",
     "iopub.status.busy": "2022-11-21T04:09:47.416227Z",
     "iopub.status.idle": "2022-11-21T04:09:47.424180Z",
     "shell.execute_reply": "2022-11-21T04:09:47.423203Z",
     "shell.execute_reply.started": "2022-11-21T04:09:47.416575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweet_id', 'emotion', '_score', 'text', '_crawldate', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
       "        num_rows: 1455563\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:09:47.426130Z",
     "iopub.status.busy": "2022-11-21T04:09:47.425412Z",
     "iopub.status.idle": "2022-11-21T04:09:48.097228Z",
     "shell.execute_reply": "2022-11-21T04:09:48.096292Z",
     "shell.execute_reply.started": "2022-11-21T04:09:47.426075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet_id', 'emotion', '_score', 'text', '_crawldate', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 1310006\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet_id', 'emotion', '_score', 'text', '_crawldate', '__index_level_0__', 'input_ids', 'attention_mask', 'label'],\n",
      "        num_rows: 145557\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_val_dataset = processed_data['train'].train_test_split(test_size=0.1)\n",
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:09:48.101165Z",
     "iopub.status.busy": "2022-11-21T04:09:48.100823Z",
     "iopub.status.idle": "2022-11-21T04:09:55.113205Z",
     "shell.execute_reply": "2022-11-21T04:09:55.112388Z",
     "shell.execute_reply.started": "2022-11-21T04:09:48.101139Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:09:55.114680Z",
     "iopub.status.busy": "2022-11-21T04:09:55.114235Z",
     "iopub.status.idle": "2022-11-21T04:10:02.419148Z",
     "shell.execute_reply": "2022-11-21T04:10:02.417504Z",
     "shell.execute_reply.started": "2022-11-21T04:09:55.114653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                           num_labels = LABEL_COUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:10:02.422642Z",
     "iopub.status.busy": "2022-11-21T04:10:02.422150Z",
     "iopub.status.idle": "2022-11-21T04:10:02.641474Z",
     "shell.execute_reply": "2022-11-21T04:10:02.640414Z",
     "shell.execute_reply.started": "2022-11-21T04:10:02.422596Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import optimization\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \".//model/\"\n",
    "optimizer = Adam(model.parameters(),\n",
    "                 betas = (0.9, 0.98),\n",
    "                 eps = 1.0e-9)\n",
    "LEARNING_RATE = optimizer\n",
    "# LEARNING_RATE = 2e-5\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "EPOCH = 6\n",
    "SAVE_LIMIT = 5\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "#     learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    save_total_limit  = SAVE_LIMIT \n",
    "    # you can set more parameters here if you want\n",
    ")\n",
    "\n",
    "# now give all the information to a trainer\n",
    "trainer = Trainer(\n",
    "   \n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_val_dataset [\"train\"],\n",
    "    eval_dataset=train_val_dataset [\"test\"],\n",
    "    # å¯ä»¥çœç•¥ï¼Œé»˜è®¤çš„data_collatorå°±æ˜¯DataCollatorWithPadding\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:10:02.643876Z",
     "iopub.status.busy": "2022-11-21T04:10:02.642758Z",
     "iopub.status.idle": "2022-11-21T04:10:02.650321Z",
     "shell.execute_reply": "2022-11-21T04:10:02.648741Z",
     "shell.execute_reply.started": "2022-11-21T04:10:02.643845Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# 29176ace88bf095f1f95e5d56fc4ea4992ce9480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:10:02.653340Z",
     "iopub.status.busy": "2022-11-21T04:10:02.652842Z",
     "iopub.status.idle": "2022-11-21T04:10:02.663684Z",
     "shell.execute_reply": "2022-11-21T04:10:02.662119Z",
     "shell.execute_reply.started": "2022-11-21T04:10:02.653265Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.save_pretrained('.//model/')  # save\n",
    "# tokenizer.save_pretrained('.//model/')  # save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-21T04:12:34.579079Z",
     "iopub.status.busy": "2022-11-21T04:12:34.578098Z",
     "iopub.status.idle": "2022-11-21T04:12:34.643352Z",
     "shell.execute_reply": "2022-11-21T04:12:34.641434Z",
     "shell.execute_reply.started": "2022-11-21T04:12:34.579042Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./model/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./model/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./model/')\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T04:10:05.696286Z",
     "iopub.status.idle": "2022-11-21T04:10:05.696699Z",
     "shell.execute_reply": "2022-11-21T04:10:05.696529Z",
     "shell.execute_reply.started": "2022-11-21T04:10:05.696508Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.merge(test,y)\n",
    "\n",
    "tmp = [test_data['text'][i] for i in range(len(test_data['text']))]\n",
    "input = tokenizer( tmp  , truncation=True, padding=True , return_tensors=\"pt\")\n",
    "# Get the output\n",
    "logits = mymodel(**input).logits\n",
    "logits\n",
    "\n",
    "# get predictions\n",
    "\n",
    "predicts = nn.functional.softmax(logits, dim = -1)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T04:10:05.698408Z",
     "iopub.status.idle": "2022-11-21T04:10:05.698823Z",
     "shell.execute_reply": "2022-11-21T04:10:05.698640Z",
     "shell.execute_reply.started": "2022-11-21T04:10:05.698621Z"
    }
   },
   "outputs": [],
   "source": [
    "result = torch.argmax(predicts, dim=1)\n",
    "print(result)\n",
    "inverted_result = labelencoder.inverse_transform(result) \n",
    "print(inverted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-11-21T04:10:05.700072Z",
     "iopub.status.idle": "2022-11-21T04:10:05.700389Z",
     "shell.execute_reply": "2022-11-21T04:10:05.700246Z",
     "shell.execute_reply.started": "2022-11-21T04:10:05.700231Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data\n",
    "res = pd.concat([ pd.DataFrame(test_data['tweet_id'].values.tolist(),columns=['id']) ,pd.DataFrame(inverted_result  ,columns=['emotion'])],axis=1 ,)\n",
    "\n",
    "\n",
    "res.to_csv('../ans.csv', encoding = 'utf-8',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
